---
title: "Pulsar"
output: html_document
date: '2022-06-28'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#Loading and Cleaning Data

pulsar_train <-  read.csv(file="C:/Users/icon8/Desktop/ME315 Project/Project Data/pulsar_data_train.csv", header=T)
head(pulsar_train)
str(pulsar_train)
dim(pulsar_train)
sum(is.na(pulsar_train))

pulsar_test <-  read.csv(file="C:/Users/icon8/Desktop/ME315 Project/Project Data/pulsar_data_test.csv", header=T)
head(pulsar_test)
str(pulsar_test)
dim(pulsar_test)
sum(is.na(pulsar_test))
```

```{r}
#Binding test and train to explore data and later make our own split
pulsar_unshuffled <- rbind(pulsar_train, pulsar_test)
head(pulsar_unshuffled)
str(pulsar_unshuffled)
dim(pulsar_unshuffled)
sum(is.na(pulsar_unshuffled))

#Rename Columns
names(pulsar_unshuffled)
names(pulsar_unshuffled) <-  c("mean_int", "std_int", "kurt_int", "skew_int","mean_dm" ,"std_dm", "kurt_dm", "skew_dm", "is_pulsar")

#Convert binary predictor from numeric to factor
pulsar_unshuffled$is_pulsar <- as.factor(pulsar_unshuffled$is_pulsar)
str(pulsar_unshuffled)

#NA Distribution
col_na <- colSums(apply(is.na(pulsar_unshuffled),2,as.numeric))
library("RColorBrewer")
colours <- brewer.pal(n = 12, name = "Paired")
barplot(sort(col_na,decreasing=T), las=2, col = colours[2], main="Number of NA values") 

#Omit NA 
pulsar_unshuffled <- na.omit(pulsar_unshuffled)
sum(is.na(pulsar_unshuffled))

#Reshuffle pulsar to make sure
set.seed(42)
reshuffle <-  sample(1:nrow(pulsar_unshuffled), nrow(pulsar_unshuffled)) #random index
pulsar <-  pulsar_unshuffled[reshuffle,]
head(pulsar)
str(pulsar)
dim(pulsar)
sum(is.na(pulsar))

pulsar_unshuffled[70:80,7] - pulsar[70:80,7] #=/= 0 
```

```{r}
###Exploratory Analysis/Transformation
str(pulsar)
summary(pulsar)
pairs(pulsar, col = colours[2])

```

```{r}
#Feature distributions

names(pulsar) #"mean_int"  "std_int"   "kurt_int"  "skew_int"  "mean_dm"   "std_dm"    "kurt_dm"   "skew_dm" "is_pulsar"
mean_int <- pulsar$mean_int
std_int <- pulsar$std_int
kurt_int <- pulsar$kurt_int
skew_int <- pulsar$skew_int
mean_dm <- pulsar$mean_dm
std_dm <- pulsar$std_dm
kurt_dm <- pulsar$kurt_dm
skew_dm <- pulsar$skew_dm

par(mfrow=c(2,4))
  hist(mean_int, col = colours[2])
  hist(std_int, col = colours[2])
  hist(kurt_int, col = colours[2])
  hist(skew_int, col = colours[2])
  hist(mean_dm, col = colours[2])
  hist(std_dm, col = colours[2])
  hist(kurt_dm, col = colours[2])
  hist(skew_dm, col = colours[2])

```

```{r}

#Feature transformations

hist(pulsar[,1], col = colours[2])

hist(pulsar[,2], col = colours[2])
hist(log(pulsar[,2]), col = colours[2]) #Subtle difference, so we test with Shapiro-Wilk
shapiro.test(pulsar[1:4000,2])
shapiro.test(log(pulsar[1:4000,2])) #--> keep original distribution, higher W

hist(pulsar[,3], col = colours[2])
hist(log(pulsar[,3]), col = colours[2])

hist(pulsar[,4], col = colours[2])
hist(log(pulsar[,4]), col = colours[2])

hist(pulsar[,5], col = colours[2])
hist(log(pulsar[,5]), col = colours[2])

hist(pulsar[,6], col = colours[2])
hist(log(pulsar[,6]), col = colours[2])

hist(pulsar[,7], col = colours[2])

hist(pulsar[,8], col = colours[2])
hist(log(pulsar[,8]), col = colours[2])
shapiro.test(pulsar[1:4000,8])
shapiro.test(log(pulsar[1:4000,8])) #subtle, keep transformation

#We see that columns 3,4,5,6, 8 have more normal distributions when transformed with log
names(pulsar)[c(3,4,5,6,8)]
sum(pulsar[c(8)]<0)
#But only 5,6 are strictly nonnegative
names(pulsar)[c(3,4,8)]
names(pulsar)[c(5,6)]
min(pulsar[,3])
#Let's add a small constant to be able to transform it to log. We lose interpretability, but may see better predictions.

a <- 0.001 #small constant
pulsar[,3] <- pulsar[,3] + abs(min(pulsar[,3])) + a
abs(min(pulsar[,3])) + a 
pulsar[,4] <- pulsar[,4] + abs(min(pulsar[,4])) + a
abs(min(pulsar[,4])) + a 
pulsar[,8] <- pulsar[,8] + abs(min(pulsar[,8])) + a
abs(min(pulsar[,8])) + a 
summary(pulsar)

#Mutate the transformed columns (3,4,5,6,8)
library(tidyverse)
pulsar <- pulsar %>%
    mutate(logmean_dm = log(mean_dm), logstd_dm = log(std_dm), logkurt_int = log(kurt_int), logskew_int = log(skew_int), logskew_dm = log(skew_dm), .keep = "unused")
names(pulsar)

pulsar <- pulsar %>% 
  dplyr::select(mean_int, std_int, logkurt_int, logskew_int, logmean_dm, logstd_dm, kurt_dm, logskew_dm, is_pulsar)
str(pulsar)
sum(is.na(pulsar))

```

```{r}
#Number of pulsars - barplot
number_pulsars <-  sum(as.numeric(pulsar$is_pulsar)-1)
number_no_pulsar <- nrow(pulsar) - number_pulsars
barplot(c(number_pulsars,number_no_pulsar), names.arg= c("pulsar","no pulsar"), col = colours[2])
#highly imbalanced data --> skewed misclassification costs, and asymmetric sensitivity/specificity

```


```{r}
#Transformed histograms
names(pulsar) #"mean_int"    "std_int"     "logkurt_int" "logskew_int" "logmean_dm"  "logstd_dm"   "kurt_dm" "logskew_dm"  "is_pulsar"  

mean_int <- pulsar$mean_int
std_int <- pulsar$std_int
logkurt_int <- pulsar$logkurt_int
logskew_int <- pulsar$logskew_int
logmean_dm <- pulsar$logmean_dm
logstd_dm <- pulsar$logstd_dm
kurt_dm <- pulsar$kurt_dm
logskew_dm <- pulsar$logskew_dm

par(mfrow=c(2,4))
  hist(mean_int, col = colours[2])
  hist(std_int, col = colours[2])
  hist(logkurt_int, col = colours[2])
  hist(logskew_int, col = colours[2])
  hist(logmean_dm, col = colours[2])
  hist(logstd_dm, col = colours[2])
  hist(kurt_dm, col = colours[2])
  hist(logskew_dm, col = colours[2])

```

```{r}

#box plots - class variable relationships
names(pulsar)
par(mfrow = c(2, 4))
  boxplot(mean_int ~ is_pulsar, data = pulsar, col = rainbow(2))
  boxplot(std_int ~ is_pulsar, data = pulsar, col = rainbow(2))
  boxplot(logkurt_int ~ is_pulsar, data = pulsar, col = rainbow(2))
  boxplot(logskew_int ~ is_pulsar, data = pulsar, col = rainbow(2))
  boxplot(logmean_dm ~ is_pulsar, data = pulsar, col = rainbow(2))
  boxplot(logstd_dm ~ is_pulsar, data = pulsar, col = rainbow(2))
  boxplot(kurt_dm ~ is_pulsar, data = pulsar, col = rainbow(2))
  boxplot(logskew_dm ~ is_pulsar, data = pulsar, col = rainbow(2))

```

```{r}
#Check for Multicolinearity:  Significance of individual Linear Regressions and corrplot 

p_val <- c()

lreg <- lm(mean_int ~ is_pulsar, data=pulsar)
  p_val <- append(p_val, summary(lreg)$coefficients[2,4])
lreg <- lm(std_int ~ is_pulsar, data=pulsar)
  p_val <- append(p_val, summary(lreg)$coefficients[2,4])
lreg <- lm(logkurt_int ~ is_pulsar, data=pulsar)
  p_val <- append(p_val, summary(lreg)$coefficients[2,4])
lreg <- lm(logskew_int ~ is_pulsar, data=pulsar)
  p_val <- append(p_val, summary(lreg)$coefficients[2,4])
lreg <- lm(logmean_dm ~ is_pulsar, data=pulsar)
  p_val <- append(p_val, summary(lreg)$coefficients[2,4])
lreg <- lm(logstd_dm ~ is_pulsar, data=pulsar)
  p_val <- append(p_val, summary(lreg)$coefficients[2,4])
lreg <- lm(kurt_dm ~ is_pulsar, data=pulsar)
  p_val <- append(p_val, summary(lreg)$coefficients[2,4])
lreg <- lm(logskew_dm ~ is_pulsar, data=pulsar)
  p_val <- append(p_val, summary(lreg)$coefficients[2,4])

p_val

#Correlation Heat Map
pulsar_numeric <- cbind(pulsar[,-9], as.numeric(pulsar[,9]))
names(pulsar_numeric)[9] = "is_pulsar"
library("corrplot")
corrplot(cor(pulsar_numeric), method = "number")

```


```{r}
#Creating our own test/train split: 60/40

set.seed(42)

train.size <-  nrow(pulsar) * 0.60
train <-  sample(1:nrow(pulsar), train.size)   #Picking random row indexes
test <-  -train
pulsar_train <-  pulsar[train, ]
pulsar_test <-  pulsar[test, ]
dim(pulsar_train)
dim(pulsar_test)
```

```{r}
###Classification
```

```{r}
#Linear Discriminant Analysis (LDA)

names(pulsar)
library(MASS)
set.seed(42)
lda.fit <-  lda(is_pulsar ~., data = pulsar, subset=train)
lda.pred <-  predict(lda.fit, pulsar_test)
table(lda.pred$class)
lda.fit$scaling
str(lda.pred)  #Threshold is a half by default. To change: su(df.pred$psterior[,1] >=.5) - R github, discriminant analysis
incorrect <- mean(lda.pred$class != pulsar_test$is_pulsar) #Calculating the proportion of incorrect
cat(c("Prediction Accuracy:"),(1-incorrect)*100,c("%"))
balanced_accuracy(lda.pred$class,pulsar_test$is_pulsar, 1,100)

```

```{r}
#IGNORE THIS CELL - the Workings for building the balanced_accuracy function; skip to next cell and run

library(InformationValue)
optimalCutoff(pulsar_test$is_pulsar, as.numeric(lda.pred$class)) #Returns cutoff that gives minimum misclassification error
misClassError(pulsar_test$is_pulsar, as.numeric(lda.pred$class), threshold = 1.01)

bin = c()
for (i in seq(0,1.5,length=100)) {
  a <- misClassError(pulsar_test$is_pulsar, as.numeric(lda.pred$class), threshold = i)
  bin = append(bin, a)
}
bin
plot(bin)

library(pROC)
plot.roc(pulsar_test$is_pulsar,as.numeric(lda.pred$class))

library(caret)
?confusionMatrix
Confusion_Matrix <- caret::confusionMatrix(lda.pred$class, pulsar_test$is_pulsar)
Confusion_Matrix
#Specificity=Predicted no pulsar | No pulsar
#False positive = predicted pulsar | no pulsar = 1-Spec
#Sensitivty=Predicted pulsar | pulsar
#False negative = Predicted no pulsar | pulsar = 1-Sens
#Balanced Accuracy = Specificity + Sensitivty /2

#If the cost of false positive is high, we don't want there to be many false positives, so we want specificity to be high. We care more about specificity than sensitivty.

sensitivity <-  Confusion_Matrix$table[1,1]/(Confusion_Matrix$table[1,1]+Confusion_Matrix$table[2,1])
specificity <- Confusion_Matrix$table[2,2]/(Confusion_Matrix$table[1,2]+Confusion_Matrix$table[2,2])
specificity
sensitivity


cost_state <- c("cost of False Negative", "cost of False Positive")
cost_factor <- c(1,5)
cost_matrix <- data.frame(cost_state, cost_factor )
accuracy_weights <- cost_matrix$cost_factor / sum(cost_matrix$cost_factor)
accuracy_weights
balanced_accuracy <- accuracy_weights %*% c(sensitivity, specificity)
balanced_accuracy
```

```{r}

#Final Function to run

library(caret)

balanced_accuracy <- function(pred_class, test_vector, FalseNegCost, FalsePosCost) {
  Confusion_Matrix <- caret::confusionMatrix(pred_class, as.factor(test_vector))
  sensitivity <-  Confusion_Matrix$table[1,1]/(Confusion_Matrix$table[1,1]+Confusion_Matrix$table[2,1])
  specificity <- Confusion_Matrix$table[2,2]/(Confusion_Matrix$table[1,2]+Confusion_Matrix$table[2,2])
  cost_state <- c("cost of False Negative", "cost of False Positive")
  cost_factor <- c(FalseNegCost,FalsePosCost)
  cost_matrix <- data.frame(cost_state, cost_factor )
  accuracy_weights <- cost_matrix$cost_factor / sum(cost_matrix$cost_factor)
  balance_accuracy <- accuracy_weights %*% c(sensitivity, specificity)
  cat("Balanced Accuracy:", balance_accuracy*100,"%")
}
balanced_accuracy(lda.pred$class, pulsar_test$is_pulsar, 1, 10)

```

```{r}
#Quadratic Discriminant Analysis (QDA)

library(MASS)
set.seed(41)
qda.fit <-  qda(is_pulsar ~ ., data = pulsar, subset = train)
qda.pred <-  predict(qda.fit, pulsar_test)
incorrect <- mean(qda.pred$class != pulsar_test$is_pulsar) 
cat(c("Prediction Accuracy:"),(1-incorrect)*100,c("%"))

balanced_accuracy(qda.pred$class,pulsar_test$is_pulsar, 1,100)
```

```{r}
#Logistic Regression

set.seed(42)
glm.fit <-  glm(is_pulsar ~ ., data = pulsar, family = binomial, subset = train)
glm.probs <-  predict(glm.fit, pulsar_test, type = "response")
glm.pred <-  rep(0, length(glm.probs))
glm.pred[glm.probs > 0.5] <- 1

incorrect <- mean(glm.pred != pulsar_test$is_pulsar) 
cat(c("Prediction Accuracy:"),(1-incorrect)*100,c("%"))

#Finding optimal threshold

set.seed(42)
bin = c()
for (i in seq(0,1,length=100)) {
  glm.pred <-  rep(0, length(glm.probs))
  glm.pred[glm.probs > i] <- 1
  incorrect <- mean(glm.pred != pulsar_test$is_pulsar) 
  bin <-  append(bin, (1-incorrect)*100)
}


summary(bin)
bin_index <- cbind(seq(0,1,length=100),bin)
which(bin_index == max(bin_index), arr.ind = TRUE)  
bin_index[40,]

plot((bin_index), ylab = "Prediction Accuracy (%)", xlab = "Threshold", col= colours[2])

#Logistic, with optimal threshold

glm.fit <-  glm(is_pulsar ~ ., data = pulsar, family = binomial, subset = train)
glm.probs <-  predict(glm.fit, pulsar_test, type = "response")
glm.pred <-  rep(0, length(glm.probs))
glm.pred[glm.probs > 0.3939394] <- 1

incorrect <- mean(glm.pred != pulsar_test$is_pulsar) 
cat(c("Prediction Accuracy:"),(1-incorrect)*100,c("%"))


balanced_accuracy(as.factor(glm.pred), pulsar_test$is_pulsar, 1, 100)

```

```{r}
#KNN Classification

library(class)

set.seed(42)
sd_pulsar_train <- scale(pulsar_train[,-9])
sd_pulsar_test <- scale(pulsar_test[,-9])
train_label <- c(pulsar_train[,9])
dim(pulsar_train)

knn.pred <-  knn(sd_pulsar_train, sd_pulsar_test, train_label, k = 1)
mean(knn.pred != pulsar_test$is_pulsar)

incorrect <- mean(knn.pred != pulsar_test$is_pulsar)
cat(c("Prediction Accuracy:"),(1-incorrect)*100,c("%"))

balanced_accuracy(knn.pred, pulsar_test$is_pulsar, 1, 5)

#Find Optimal K

set.seed(42)
bin <- c()
for (i in 1:100) {
  knn.pred <-  knn(sd_pulsar_train, sd_pulsar_test, train_label, k = i)
  accuracy <- 1-mean(knn.pred != pulsar_test$is_pulsar)
  bin <- append(bin,accuracy)
}
bin

k_bin <- cbind(seq(1,100), bin)
plot(k_bin, ylab = "Prediction Accuracy (%)", xlab = "Number of Neighbors (K)", col= colours[2])
k_bin
which(k_bin[,2] == max(k_bin[,2]), arr.ind = T)  
k_bin[13,]

#KNN, with optimal K

knn.pred <-  knn(sd_pulsar_train, sd_pulsar_test, train_label, k = 13)
mean(knn.pred != pulsar_test$is_pulsar)

incorrect <- mean(knn.pred != pulsar_test$is_pulsar)
cat(c("Prediction Accuracy:"),(1-incorrect)*100,c("%"))

balanced_accuracy(as.factor(knn.pred), pulsar_test$is_pulsar, 1, 100)
```


```{r}
###Classification Model Evaluation 


#Simple, Balanced Accuracies

incorrect <- mean(lda.pred$class != pulsar_test$is_pulsar)
cat(c("Prediction Accuracy:"),(1-incorrect)*100,c("%"))

incorrect <- mean(qda.pred$class != pulsar_test$is_pulsar) 
cat(c("Prediction Accuracy:"),(1-incorrect)*100,c("%"))

incorrect <- mean(glm.pred != pulsar_test$is_pulsar) 
cat(c("Prediction Accuracy:"),(1-incorrect)*100,c("%"))

incorrect <- mean(knn.pred != pulsar_test$is_pulsar)
cat(c("Prediction Accuracy:"),(1-incorrect)*100,c("%"))


balanced_accuracy(lda.pred$class,pulsar_test$is_pulsar, 1,100)
balanced_accuracy(qda.pred$class,pulsar_test$is_pulsar, 1,100)
balanced_accuracy(as.factor(glm.pred), pulsar_test$is_pulsar, 1, 100)
balanced_accuracy(as.factor(knn.pred), pulsar_test$is_pulsar, 1, 100)

#Overlaid ROC curves and AUC

library(pROC)

roc_lda <- roc(pulsar_test$is_pulsar, as.numeric(lda.pred$class)-1)
roc_qda <- roc(pulsar_test$is_pulsar,as.numeric(qda.pred$class)-1)
roc_glm <- roc(pulsar_test$is_pulsar, as.numeric(glm.pred))
roc_knn <- roc(pulsar_test$is_pulsar, as.numeric(knn.pred))

plot.roc(pulsar_test$is_pulsar,as.numeric(qda.pred$class)-1, ci=T, of="sp",type="bar") #black
  lines(roc_lda, ci=T, of="sp", type="bar",col=2) #red
  lines(roc_glm, ci=T, of="sp", type="bar",col=3) #green
  lines(roc_knn, ci=T, of="sp", type="bar",col=4) #blue
?auc
auc(roc_lda)
auc(roc_qda)
auc(roc_glm)
auc(roc_knn)
```


```{r}
###Clustering 
```

```{r}
##Hierarchical Clustering 
```

```{r}
#Complete HClust Linkage 

#data pre-processing
set.seed(42)

sd_pulsar_train <- scale(pulsar_train[,-9])
sd_pulsar_test <- scale(pulsar_test[,-9])
train_label <- c(pulsar_train[,9])
head(sd_pulsar_test)

#distance matrix
dist_sd_pulsar_train =dist(sd_pulsar_train)
summary(dist_sd_pulsar_train)

#Complete Linkage - highest accuracy
set.seed(42)
complete_link <- hclust(dist_sd_pulsar_train, method="complete")
plot(complete_link, labels=train_label, main="Complete Linkage", xlab="", sub="",ylab="")

cut_dendro <- cut(as.dendrogram(complete_link), h=12)
par(mfrow=c(1, 3))
plot(cut_dendro$lower[[1]], main = "First branch of lower tree at cut height h=12")
plot(cut_dendro$lower[[2]], main = "Second branch of lower tree at cut height h=12")
plot(cut_dendro$upper, main = "Upper tree at cut height h=12")
#Dendrogram tree is not that informative

set.seed(42)
complete_binary <- cutree(complete_link,2)-1
table(complete_binary, train_label)

incorrect <- mean(complete_binary != train_label) 
cat(c("Prediction Accuracy:"),(1-incorrect)*100,c("%"))

balanced_accuracy(as.factor(complete_binary),train_label, 1,100)

```


```{r}
#AverageLinkage

set.seed(42)
average_link <- hclust(dist_sd_pulsar_train, method="average")
plot(average_link, labels=train_label, main="Average Linkage", xlab="", sub="",ylab="")

cut_dendro <- cut(as.dendrogram(average_link), h=2)
par(mfrow=c(1, 3))
plot(cut_dendro$lower[[1]], main = "First branch of lower tree at cut height h=12")
plot(cut_dendro$lower[[2]], main = "Second branch of lower tree at cut height h=12")
plot(cut_dendro$upper, main = "Upper tree at cut height h=12")

set.seed(42)
average_binary <- cutree(average_link,2)-1
table(average_binary, train_label)

incorrect1 <- mean(average_binary != train_label) 
cat(c("Prediction Accuracy:"),(1- incorrect1)*100,c("%"))
balanced_accuracy(as.factor(average_binary),train_label, 1,100)

#Confusion matrix to make sure f:balanced_accuracies is working properly
library(caret)
  Confusion_Matrix <- caret::confusionMatrix(as.factor(complete_binary), as.factor(train_label))
Confusion_Matrix  

```

```{r}
#Single Linkage

set.seed(42)
single_link <- hclust(dist_sd_pulsar_train, method="single")
plot(single_link, labels=train_label, main="Single Linkage", xlab="", sub="",ylab="")

cut_dendro <- cut(as.dendrogram(single_link), h=2)
par(mfrow=c(1, 3))
plot(cut_dendro$lower[[1]], main = "First branch of lower tree at cut height h=12")
plot(cut_dendro$lower[[2]], main = "Second branch of lower tree at cut height h=12")
plot(cut_dendro$upper, main = "Upper tree at cut height h=12")

set.seed(42)
single_binary <- cutree(single_link,2)-1
table(single_binary, train_label)

incorrect2 <- mean(single_binary != train_label) 
cat(c("Prediction Accuracy:"),(1-incorrect2)*100,c("%"))
balanced_accuracy(as.factor(single_binary),train_label, 1,100)

library(caret)
  Confusion_Matrix <- caret::confusionMatrix(as.factor(single_binary), as.factor(train_label))
Confusion_Matrix  
```


```{r}
#K-means Clustering

sd_pulsar_train <- scale(pulsar_train[,-9])
sd_pulsar_test <- scale(pulsar_test[,-9])
train_label <- c(pulsar_train[,9])
head(sd_pulsar_test)

set.seed(42)

km_out <- kmeans(sd_pulsar_train, 2, nstart=20)
km_class <- km_out$cluster
plot(sd_pulsar_train, col=(km_class+8), main="K-Means Clustering Results with K=2", xlab="", ylab="", pch=20, cex=2)

set.seed(42)
km_binary <- km_class - 1
table(km_binary, train_label)

incorrect3 <- mean(km_binary != train_label) 
cat(c("Prediction Accuracy:"),(incorrect3)*100,c("%"))
balanced_accuracy(as.factor(km_binary),train_label,1,10)

pairs(sd_pulsar_train, lower.panel = NULL, col = (km_class), main="K-Means")

library(caret)
  Confusion_Matrix <- caret::confusionMatrix(as.factor(km_binary), as.factor(train_label))

```

```{r}
#Gaussian Mixture

sd_pulsar_train <- scale(pulsar_train[,-9])
sd_pulsar_test <- scale(pulsar_test[,-9])
train_label <- c(pulsar_train[,9])
head(sd_pulsar_test)


set.seed(42)
library(mclust)
Model <- Mclust(sd_pulsar_train,G=2,startCL = "kmeans", modelNames=c("EII","VII"))
?mclust
plot(Model,sd_pulsar_train,what="BIC")
summary(Model)
plot(sd_pulsar_train, col=(Model$classification+16), main="Gaussian Mixtures", xlab="", ylab="", pch=20, cex=2)
set.seed(42)
gclassed <- Model$classification-1
table(gclassed, train_label) #1 is the pulsar cluster


incorrect4 <- mean(gclassed != train_label) 
cat(c("Prediction Accuracy:"),(1-incorrect4)*100,c("%"))
balanced_accuracy(as.factor(gclassed),train_label, 1,100)

pairs(sd_pulsar_train, lower.panel = NULL, col = (Model$classification), main="Gaussian Mixtures")

```


```{r}
###Evaluating Clusters

#Compare the Hclusters

incorrect <- mean(complete_binary != train_label) 
cat(c("Prediction Accuracy:"),(1-incorrect)*100,c("%"))

incorrect <- mean(average_binary != train_label) 
cat(c("Prediction Accuracy:"),(1-incorrect)*100,c("%"))

incorrect <- mean(single_binary != train_label) 
cat(c("Prediction Accuracy:"),(1-incorrect)*100,c("%"))

balanced_accuracy(as.factor(complete_binary),train_label, 1,100)
balanced_accuracy(as.factor(average_binary),train_label, 1,100)
balanced_accuracy(as.factor(single_binary),train_label, 1,100)

#Compare KNN and Gaussian Mixture

incorrect <- mean(km_binary != train_label) 
cat(c("Prediction Accuracy:"),(1-incorrect)*100,c("%"))

incorrect <- mean(classed != train_label)    #Gaussian
cat(c("Prediction Accuracy:"),(1-incorrect)*100,c("%"))

balanced_accuracy(as.factor(km_binary),train_label, 1,100)
balanced_accuracy(as.factor(gclassed),train_label, 1,100)
#Possible cluster-classify combinations: Logistic & Kmeans QDA and Gauss


#ROC, AUC Evalutation

library(pROC)

roc_comp <- roc(train_label, complete_binary)
roc_avg <- roc(train_label, average_binary)
roc_sing <- roc(train_label, single_binary)
roc_km <- roc(train_label, km_binary)
roc_gm <- roc(train_label, gclassed)
?roc

plot.roc(train_label, complete_binary, ci=T, of="sp",type="bar") #black
  lines(roc_avg, ci=T, of="sp", type="bar",col=3) #green
  lines(roc_sing, ci=T, of="sp", type="bar",col=4) #blue
  lines(roc_km, ci=T, of="sp", type="bar",col=5) #light blue
  lines(roc_gm, ci=T, of="sp", type="bar",col=6) #purple
  
auc(roc_comp)
auc(roc_avg)
auc(roc_sing)
auc(roc_km)
auc(roc_gm)

```

```{r}
###Cluster-then-Classify

```

```{r}
##Gaussian Mix-then-QDA
```

```{r}
#Data prep
sd_pulsar_train <- scale(pulsar_train[,-9])
sd_pulsar_test <- scale(pulsar_test[,-9])
train_label <- c(pulsar_train[,9])
test_label <- c(pulsar_test[,9])


#Recycle training cluster class from earlier
sd_ptc <- data.frame(cbind(sd_pulsar_train,pulsar_train[9],as.factor(gclassed)))
str(sd_ptc)
names(sd_ptc)[10] <- "classed"
names(sd_ptc)
```

```{r}
#Gaussian Mixture Cluster 

set.seed(42)
library(mclust)
Model <- Mclust(sd_pulsar_test,G=2,startCL = "kmeans", modelNames=c("EII","VII"))
plot(Model,sd_pulsar_test,what="BIC")
summary(Model)
plot(sd_pulsar_test, col=(Model$classification+16), main="Gaussian Mixtures", xlab="", ylab="", pch=20, cex=2)
classed2 <- Model$classification-1
length(classed2)
length(test_label)
table(classed2, test_label)

incorrect <- mean(classed2 != pulsar_test$is_pulsar)
cat(c("Prediction Accuracy:"),(1-incorrect)*100,c("%"))

sd_ptestc <- data.frame(cbind(sd_pulsar_test,pulsar_test[9],as.factor(classed2)))
str(sd_ptestc)
names(sd_ptestc)[10] <- "classed"
names(sd_ptestc)
names(sd_ptc)
dim(sd_ptc)
dim(sd_ptestc)
```

```{r}
#Quadratic Discriminant Analysis (QDA)
library(MASS)
qda.fit2 <-  qda(is_pulsar ~ . + mean_int*classed + std_int*classed + logkurt_int*classed + logskew_int*classed + logmean_dm*classed + logstd_dm*classed + kurt_dm*classed + logskew_dm*classed, data = sd_ptc)
qda.pred2 <-  predict(qda.fit2, sd_ptestc)

incorrect <- mean(qda.pred2$class != pulsar_test$is_pulsar) 
cat(c("Prediction Accuracy:"),(1-incorrect)*100,c("%"))

balanced_accuracy(qda.pred2$class,pulsar_test$is_pulsar, 1,100)
```

```{r}
## KM-then-Logistic
```

```{r}
#Data prep

sd_pulsar_train <- scale(pulsar_train[,-9])
sd_pulsar_test <- scale(pulsar_test[,-9])
train_label <- c(pulsar_train[,9])
test_label <- c(pulsar_test[,9])
head(sd_pulsar_test)

sd_ptc <- data.frame(cbind(sd_pulsar_train,pulsar_train[9],as.factor(km_binary)))
str(sd_ptc)
names(sd_ptc)[10] <- "classed"
names(sd_ptc)

```

```{r}

#cluster unlabeled test data

set.seed(42)
km_out <- kmeans(sd_pulsar_test, 2, nstart=20)
km_class2 <- km_out$cluster
plot(sd_pulsar_test, col=(km_class2+8), main="K-Means Clustering Results with K=2", xlab="", ylab="", pch=20, cex=2)

km_binary2 <- km_class2 - 1
table(km_binary2, test_label) 

incorrect5 <- mean(km_binary2 != test_label) 
cat(c("Prediction Accuracy:"),(1-incorrect)*100,c("%"))

```


```{r}
#Logistic regression
set.seed(42)
sd_ptestc <- data.frame(cbind(sd_pulsar_test,pulsar_test[9],as.factor(km_binary2)))
str(sd_ptestc)
names(sd_ptestc)[10] <- "classed"
names(sd_ptestc)


glm.fit2 <-  glm(is_pulsar ~ . + mean_int*classed + std_int*classed + logkurt_int*classed + logskew_int*classed + logmean_dm*classed + logstd_dm*classed + kurt_dm*classed + logskew_dm*classed, data = sd_ptc, family = binomial)   #Warning: fitted probabilities numerically 0 or 1 occured
glm.probs2 <-  predict(glm.fit2, data.frame(sd_ptestc), type = "response")
summary(glm.fit2)
glm.pred2 <-  rep(0, length(glm.probs2))
glm.pred2[glm.probs2 > 0.5] <- 1

incorrect6 <- mean(glm.pred2 != pulsar_test$is_pulsar) 
cat(c("Prediction Accuracy:"),(1-incorrect6)*100,c("%"))

#Optimal threshold
set.seed(42)
bin = c()
for (i in seq(0,1,length=100)) {
  glm.pred2 <-  rep(0, length(glm.probs2))
  glm.pred2[glm.probs2 > i] <- 1
  incorrect <- mean(glm.pred2 != pulsar_test$is_pulsar) 
  bin <-  append(bin, (1-incorrect)*100)
}

bin_index <- cbind(seq(0,1,length=100),bin)
which(bin_index == max(bin_index), arr.ind = TRUE)  
bin_index[40,]
#0.3939394

#Logistic, with new threshold
set.seed(42)
glm.fit2 <-  glm(is_pulsar ~ . + mean_int*classed + std_int*classed + logkurt_int*classed + logskew_int*classed + logmean_dm*classed + logstd_dm*classed + kurt_dm*classed + logskew_dm*classed, data = sd_ptc, family = binomial)   #Warning: fitted probabilities numerically 0 or 1 occured
glm.probs2 <-  predict(glm.fit2, data.frame(sd_ptestc), type = "response")
summary(glm.fit2)
glm.pred2 <-  rep(0, length(glm.probs2))
glm.pred2[glm.probs2 > 0.3939394] <- 1

incorrect7 <- mean(glm.pred2 != pulsar_test$is_pulsar) 
cat(c("Prediction Accuracy:"),(1-incorrect7)*100,c("%"))

balanced_accuracy(as.factor(glm.pred2), pulsar_test$is_pulsar, 1, 100)

```

```{r}
#ROC, AUC evaluation
library(pROC)


roc_qda2 <- roc(pulsar_test$is_pulsar,as.numeric(qda.pred2$class)-1)
roc_glm2 <- roc(pulsar_test$is_pulsar, as.numeric(glm.pred2))

plot.roc(pulsar_test$is_pulsar,as.numeric(qda.pred2$class)-1, ci=T, of="sp",type="bar") #black
  lines(roc_glm2, ci=T, of="sp", type="bar",col=3) #green

auc(roc_qda2)
auc(roc_glm2)

```



